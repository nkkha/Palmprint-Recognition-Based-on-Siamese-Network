{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. ROI Extraction for Palmprint Images\n",
    "Tested on Tongji dataset - Full palmprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "conda env update --file environment.yml --prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function, Variable\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import copy\n",
    "torch.set_default_dtype(torch.float64)\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from networks.ROILAnet import ROILAnet\n",
    "from networks.TPSGridGen import TPSGridGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROIModelPath = 'weights/ROI_extractor_augmented_TJ-NTU.pt' # path to the trained ROI model\n",
    "#CNNModelPath = 'weights/resnet18_tongji_unfreezed_extractor.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the tongji data set we have 600 palm samples of 300 different people.\n",
    "class_names = []\n",
    "for i in range(600):\n",
    "    class_names.append(f'user_{i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defnition of Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadROIModel(weightPath: str = None):\n",
    "    \"\"\"\n",
    "    @weightPath: path to file ROILAnet() weights\n",
    "    load localization network with pretrain weights\n",
    "    \"\"\"\n",
    "    model = ROILAnet()\n",
    "    model.load_state_dict(torch.load(weightPath, map_location=torch.device(device)))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    model.requires_grads=False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getThinPlateSpline(target_width: int = 112, target_height: int = 112) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    @target_width: desired I_ROI output width\n",
    "    @target_height: desired I_ROI output height\n",
    "    make instance of TPS grid generator\n",
    "    \"\"\"\n",
    "    # Create points on palm with TPS\n",
    "    target_control_points = torch.Tensor(list(itertools.product(\n",
    "        torch.arange(-1.0, 1.00001, 1.0),\n",
    "        torch.arange(-1.0, 1.00001, 1.0),\n",
    "    )))\n",
    "    gridgen = TPSGridGen(target_height=target_height, target_width=target_width, target_control_points=target_control_points)\n",
    "    gridgen = gridgen.to(device)\n",
    "    return gridgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOriginalAndResizedInput(path: str = None) -> (np.ndarray, torch.Tensor, torch.Tensor):\n",
    "    \"\"\"\n",
    "    @path: Image needs to be loaded from database\n",
    "     This function loads a resized image from a directory given in the path.\n",
    "     After resizing to 56x56 pixels, the original and resized images will be returned as triples (PILMain, source_image, resized)\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        return (None, None)\n",
    "    \n",
    "    # Define transformer for resized input of feature extraction CNN\n",
    "    resizeTranformer = transforms.Compose([\n",
    "            transforms.Resize((56,56)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    PILMain = Image.open(path).convert(mode = 'RGB') # load image in PIL format\n",
    "    sourceImage = np.array(PILMain).astype('float64') # convert from PIL to float64\n",
    "    sourceImage = transforms.ToTensor()(sourceImage).unsqueeze_(0) # add first dimension, which is batch dim\n",
    "    sourceImage = sourceImage.to(device) # load to available device\n",
    "\n",
    "    resizedImage = resizeTranformer(PILMain)\n",
    "    resizedImage = resizedImage.view(-1,resizedImage.size(0),resizedImage.size(1),resizedImage.size(2))\n",
    "    resizedImage = resizedImage.to(device) # load to available device\n",
    "    return (PILMain, sourceImage,resizedImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getThetaHat(resizedImage: torch.Tensor = None, model = None) -> torch.Tensor: \n",
    "    \"\"\"\n",
    "    @resizedImage: Cut image\n",
    "    @model: ROI Localisation network, generate a resized theta vector Image: the image needs to be loaded from the database via the getOriginalAndResizedInput function. \n",
    "     Here the theta vector is computed using the pre-trained network. The vector has size [9, 2] -> represents 9 pairs of x and y values\n",
    "    \"\"\"\n",
    "    if resizedImage is None:\n",
    "        return None\n",
    "    \n",
    "    with torch.no_grad(): # cancel gradients try to predict by ROI\n",
    "        theta_hat = model.forward(resizedImage)\n",
    "    theta_hat = theta_hat.view(-1, 2, 9) # split into vectors x and y -> theta_hat is initially a vector like [xxxxxxxxyyyyyyyyyy]\n",
    "    theta_hat = torch.stack((theta_hat[:,0], theta_hat[:,1]),-1)\n",
    "    return theta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleGrid(theta_hat: torch.Tensor = None, sourceImage: torch.Tensor = None, target_width: int = 112, target_height: int = 112 ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    @theta_hat: theta vector of the normalized xy coordinate pair\n",
    "    @sourceImage: the original image without cropping or resizing\n",
    "    @target_width: wide target IROI output\n",
    "    @target_height: tall target IROI output\n",
    "     Mesh samples from a given theta vector, source image, and mesh generator\n",
    "    \"\"\"\n",
    "    gridgen = getThinPlateSpline(target_width, target_height)\n",
    "    # Create grid points from the calculation of theta_hat vector\n",
    "    source_coordinate = gridgen(theta_hat)\n",
    "    # Create target grid - with target height and target width\n",
    "    grid = source_coordinate.view(-1, target_height, target_width, 2).to(device)\n",
    "    # Sample ROI from input image and generate T(theta_hat)\n",
    "    target_image = F.grid_sample(sourceImage, grid,align_corners=False)\n",
    "    return target_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printExtraction(target_image: torch.Tensor = None, source_image = None):\n",
    "    \"\"\"\n",
    "    @source_image: print source_image in PIL format\n",
    "    @target_image: print target_image as tensor (ROI)\n",
    "    \"\"\"\n",
    "    # Prepare to display -> return from gpu if needed\n",
    "    target_image = target_image.cpu().data.numpy().squeeze().swapaxes(0, 1).swapaxes(1, 2)\n",
    "    target_image = Image.fromarray(target_image.astype('uint8'))\n",
    "    plt.imshow(source_image)\n",
    "    plt.show() # show original image\n",
    "    plt.imshow(target_image)\n",
    "    plt.show() # show ROI image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadCNNModel(weightPath: str = None):\n",
    "    \"\"\"\n",
    "    @weightPath: path to ROILAnet() weights loadCNNModel file with pretrained weights\n",
    "    \"\"\"\n",
    "    model = models.resnet18(pretrained=False)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "    model.load_state_dict(torch.load(weightPath, map_location=torch.device(device)))\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIROI(model, input):\n",
    "    resizedImage = F.interpolate(input, (56, 56))\n",
    "    theta_hat = getThetaHat(resizedImage=resizedImage, model=model) # generate theta with normalized ROI\n",
    "    IROI = sampleGrid(theta_hat=theta_hat, sourceImage=input, target_width=224, target_height=224) # get ROI from original image\n",
    "    IROI.to(device)\n",
    "    return IROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markImage(image, theta):\n",
    "    nimg = np.array(image)\n",
    "    ocvim = cv2.cvtColor(nimg, cv2.COLOR_RGB2BGR)\n",
    "    for idx, coord in enumerate(theta):\n",
    "        currX = coord[0]\n",
    "        currY = coord[1]\n",
    "        x = int((ocvim.shape[1] - 1) / (1 + 1) * (currX - 1) + ocvim.shape[1])\n",
    "        y = int((ocvim.shape[0] - 1) / (1 + 1) * (currY - 1) + ocvim.shape[0])\n",
    "        ocvim = cv2.circle(ocvim,(x,y),6,(200,0,0),2)\n",
    "        ocvim = cv2.putText(\n",
    "            ocvim, # numpy array on which text is written\n",
    "            str(idx), # text\n",
    "            (x, y), # position at which writing has to start\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, # font family\n",
    "            1, # font size\n",
    "            (209, 80, 0, 255), # font color\n",
    "            3)\n",
    "    ocvim = ocvim[...,::-1]\n",
    "    return ocvim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOriginalAndResizedInput(PILMain) -> (np.ndarray, torch.Tensor, torch.Tensor):\n",
    "    \"\"\"\n",
    "    @path: image path retrieved from database\n",
    "     This function loads a resized image from a directory provided in the path.\n",
    "     After resizing to 56x56 pixels, the original and resized images will be returned.\n",
    "     as triples (PILMain, source_image, resizedImage)\n",
    "    \"\"\"\n",
    "    if PILMain is None:\n",
    "        return (None, None)\n",
    "    \n",
    "    # Define transformer for resized input of CNN feature extraction\n",
    "    resizeTranformer = transforms.Compose([\n",
    "            transforms.Resize((56,56)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # PILMain = PILMain.convert(mode = 'RGB') # load image in PIL format\n",
    "    sourceImage = np.array(PILMain).astype('float64') # convert from PIL to float64\n",
    "    sourceImage = transforms.ToTensor()(sourceImage).unsqueeze_(0) # add first dimension, which is batch dim\n",
    "    sourceImage = sourceImage.to(device) # load to available device\n",
    "\n",
    "    resizedImage = resizeTranformer(PILMain)\n",
    "    resizedImage = resizedImage.view(-1,resizedImage.size(0),resizedImage.size(1),resizedImage.size(2))\n",
    "    resizedImage = resizedImage.to(device) # load to available device\n",
    "    return (PILMain, sourceImage,resizedImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load localisation network for ROI extraction\n",
    "localisationNetwork = loadROIModel(ROIModelPath) # load localisation network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image  \n",
    "import PIL\n",
    "from torchvision.utils import save_image\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CROP ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictAndShow(path: str, pname, p_classname):\n",
    "    \"\"\"\n",
    "    Image prediction\n",
    "    @path: path to palm image in (Tongji) dataset\n",
    "    \"\"\"\n",
    "    grayTransformer = transforms.Compose([\n",
    "                    transforms.CenterCrop((224,224)),\n",
    "                    transforms.Grayscale(num_output_channels=3),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    CNNtransformer = transforms.Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.CenterCrop((224,224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3,1,1)),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    plt.figure(figsize=(20,20)) # specifying the overall grid size\n",
    " \n",
    "    # Check user 0\n",
    "    inputPIL = Image.open(path).convert('RGB')\n",
    "    # plt.subplot(4,1,1)\n",
    "    # plt.imshow(inputPIL)\n",
    "    # plt.title('Input Image')\n",
    "    classes=['user_0']\n",
    "    (PILMain, sourceImage,resizedImage) = getOriginalAndResizedInput(inputPIL)\n",
    "    sourceImage = torch.stack([sourceImage.squeeze()])\n",
    "    resizedImage = torch.stack([resizedImage.squeeze()])\n",
    "    # Get normalized coordinates\n",
    "    theta_hat = getThetaHat(resizedImage, localisationNetwork)\n",
    "    # plt.subplot(4,1,2)\n",
    "    # plt.imshow(markImage(inputPIL, theta_hat[0]))\n",
    "    # plt.title('Estimated points')\n",
    "    # Get all ROIs\n",
    "    IROI = sampleGrid(theta_hat=theta_hat, sourceImage=sourceImage, target_width=300, target_height=300)\n",
    "    IROI = IROI[0]\n",
    "    plt.subplot(4,1,3)\n",
    "\n",
    "    plt.imshow((IROI.cpu()[0]),cmap='gray')\n",
    "    # plt.title('ROI Extraction')\n",
    "    # save_image(IROI.cpu()[0], 'img1.tiff')\n",
    "    plt.axis('off')\n",
    "    # plt.tight_layout(pad=0)\n",
    "     \n",
    "    plt.savefig('ROI_session1/'+p_classname+'/'+ pname ,bbox_inches='tight',pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path1 = \"session1/\"\n",
    "path2 = \"session2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00001.tiff\n",
      "00001.tiff\n"
     ]
    }
   ],
   "source": [
    "files1= os.listdir(os.path.expanduser(path1))\n",
    "\n",
    "# files1 = files1.sort()\n",
    "files1 = sorted(files1)\n",
    "\n",
    "print(files1[0])\n",
    "files2= os.listdir(os.path.expanduser(path2))\n",
    "# Print(len(files2))\n",
    "files2 = sorted(files2)\n",
    "print(files2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split data for hand 1 of session 1\n",
    "user = 1\n",
    "hand1 = 0\n",
    "hand2 = 0\n",
    "count = 0\n",
    "\n",
    "for img in files1:\n",
    "    if (count == 0 & hand1 == 0):\n",
    "        os.mkdir('hand1_session1/' + str(user))\n",
    "        shutil.copy2('session1/' + img, 'hand1_session1/' + str(user) + '/')\n",
    "        count = count + 1\n",
    "        hand1 = hand1 + 1\n",
    "    else:\n",
    "        if (count <= 9):\n",
    "            shutil.copy2('session1/' + img, 'hand1_session1/' + str(user) + '/')\n",
    "            hand1 = hand1 + 1\n",
    "            count = count + 1\n",
    "        else:\n",
    "            if(count == 11):\n",
    "                count = count + 1\n",
    "                hand2 = hand2 + 1\n",
    "            else:\n",
    "                if(count <= 20):\n",
    "                    hand2 = hand2 + 1\n",
    "                    count = count + 1\n",
    "                    if(count == 20):\n",
    "                        count = 0\n",
    "                        hand1 = 0\n",
    "                        hand2 = 0\n",
    "                        user = user + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the image to add suffix (1) which is the image of session1\n",
    "from glob import glob\n",
    "path3 = \"session12/\"\n",
    "\n",
    "for class_ in range(1,301):\n",
    "    full_path_directory = 'session12/' + str(class_) + '/'\n",
    "    # print(full_path_directory)\n",
    "    # print(class_)\n",
    "    path3 = full_path_directory\n",
    "    files3 = os.listdir(os.path.expanduser(path3))\n",
    "    for img in files3:\n",
    "        # print(img)\n",
    "        x = img.split(\".\")\n",
    "        img_new = x[0] + \" (1).\"+x[1]\n",
    "        # print(x[0])\n",
    "        old_file = os.path.join(full_path_directory, img)\n",
    "        new_file = os.path.join(full_path_directory, img_new)\n",
    "        # print(old_file)\n",
    "        # print(new_file)\n",
    "        os.rename(old_file, new_file)\n",
    "print('Finish!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for hand 1 of session 2\n",
    "user = 1\n",
    "hand1 = 0\n",
    "hand2 = 0\n",
    "count = 0\n",
    "\n",
    "for img in files2:\n",
    "    if (count == 0 & hand1 == 0):\n",
    "        # Moving\n",
    "        shutil.copy2('session2/' + img, 'session12/' + str(user) + '/')\n",
    "        count = count + 1\n",
    "        hand1 = hand1 + 1\n",
    "    else:\n",
    "        if (count <= 9):\n",
    "            shutil.copy2('session2/' + img, 'session12/' + str(user) + '/')\n",
    "            hand1 = hand1 + 1\n",
    "            count = count + 1\n",
    "        else:\n",
    "            if(count == 11):\n",
    "                count = count + 1\n",
    "                hand2 = hand2 + 1\n",
    "            else:\n",
    "                if(count <= 20):\n",
    "                    hand2 = hand2 + 1\n",
    "                    count = count + 1\n",
    "                    if(count == 20):\n",
    "                        count = 0\n",
    "                        hand1 = 0\n",
    "                        hand2 = 0\n",
    "                        user = user + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform ROI extraction for hand 1 images, split to avoid RAM overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "for class_ in range(1,28):\n",
    "    class_imgs_path = glob(f'hand1_session1/{str(class_)}/*.tiff')\n",
    "    \n",
    "    os.mkdir('ROI_session1/' + str(class_))\n",
    "    for img in class_imgs_path:\n",
    "        result = os.path.splitext(str(os.path.basename(img)))[0]\n",
    "        # print(result + \".JPEG\")\n",
    "        predictAndShow(img, result + \".JPEG\", str(class_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "for class_ in range(28,55):\n",
    "    class_imgs_path = glob(f'hand1_session1/{str(class_)}/*.tiff')\n",
    "    \n",
    "    os.mkdir('ROI_session1/' + str(class_))\n",
    "    for img in class_imgs_path:\n",
    "        result = os.path.splitext(str(os.path.basename(img)))[0]\n",
    "        # print(result + \".JPEG\")\n",
    "        predictAndShow(img, result + \".JPEG\", str(class_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "for class_ in range(55,82):\n",
    "    class_imgs_path = glob(f'hand1_session1/{str(class_)}/*.tiff')\n",
    "    \n",
    "    os.mkdir('ROI_session1/' + str(class_))\n",
    "    for img in class_imgs_path:\n",
    "        result = os.path.splitext(str(os.path.basename(img)))[0]\n",
    "        # print(result + \".JPEG\")\n",
    "        predictAndShow(img, result + \".JPEG\", str(class_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "for class_ in range(82,109):\n",
    "    class_imgs_path = glob(f'hand1_session1/{str(class_)}/*.tiff')\n",
    "    \n",
    "    os.mkdir('ROI_session1/' + str(class_))\n",
    "    for img in class_imgs_path:\n",
    "        result = os.path.splitext(str(os.path.basename(img)))[0]\n",
    "        # print(result + \".JPEG\")\n",
    "        predictAndShow(img, result + \".JPEG\", str(class_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "for class_ in range(109,135):\n",
    "    class_imgs_path = glob(f'hand1_session1/{str(class_)}/*.tiff')\n",
    "    \n",
    "    os.mkdir('ROI_session1/' + str(class_))\n",
    "    for img in class_imgs_path:\n",
    "        result = os.path.splitext(str(os.path.basename(img)))[0]\n",
    "        # print(result + \".JPEG\")\n",
    "        predictAndShow(img, result + \".JPEG\", str(class_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "for class_ in range(135,163):\n",
    "    class_imgs_path = glob(f'hand1_session1/{str(class_)}/*.tiff')\n",
    "    \n",
    "    os.mkdir('ROI_session1/' + str(class_))\n",
    "    for img in class_imgs_path:\n",
    "        result = os.path.splitext(str(os.path.basename(img)))[0]\n",
    "        # print(result + \".JPEG\")\n",
    "        predictAndShow(img, result + \".JPEG\", str(class_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "for class_ in range(163,191):\n",
    "    class_imgs_path = glob(f'hand1_session1/{str(class_)}/*.tiff')\n",
    "    \n",
    "    os.mkdir('ROI_session1/' + str(class_))\n",
    "    for img in class_imgs_path:\n",
    "        result = os.path.splitext(str(os.path.basename(img)))[0]\n",
    "        # print(result + \".JPEG\")\n",
    "        predictAndShow(img, result + \".JPEG\", str(class_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "for class_ in range(191,219):\n",
    "    class_imgs_path = glob(f'hand1_session1/{str(class_)}/*.tiff')\n",
    "    \n",
    "    os.mkdir('ROI_session1/' + str(class_))\n",
    "    for img in class_imgs_path:\n",
    "        result = os.path.splitext(str(os.path.basename(img)))[0]\n",
    "        # print(result + \".JPEG\")\n",
    "        predictAndShow(img, result + \".JPEG\", str(class_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "for class_ in range(219,241):\n",
    "    class_imgs_path = glob(f'hand1_session1/{str(class_)}/*.tiff')\n",
    "    \n",
    "    os.mkdir('ROI_session1/' + str(class_))\n",
    "    for img in class_imgs_path:\n",
    "        result = os.path.splitext(str(os.path.basename(img)))[0]\n",
    "        # print(result + \".JPEG\")\n",
    "        predictAndShow(img, result + \".JPEG\", str(class_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "for class_ in range(241,269):\n",
    "    class_imgs_path = glob(f'hand1_session1/{str(class_)}/*.tiff')\n",
    "    \n",
    "    os.mkdir('ROI_session1/' + str(class_))\n",
    "    for img in class_imgs_path:\n",
    "        result = os.path.splitext(str(os.path.basename(img)))[0]\n",
    "        # print(result + \".JPEG\")\n",
    "        predictAndShow(img, result + \".JPEG\", str(class_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "for class_ in range(269,301):\n",
    "    class_imgs_path = glob(f'hand1_session1/{str(class_)}/*.tiff')\n",
    "    \n",
    "    os.mkdir('ROI_session1/' + str(class_))\n",
    "    for img in class_imgs_path:\n",
    "        result = os.path.splitext(str(os.path.basename(img)))[0]\n",
    "        # print(result + \".JPEG\")\n",
    "        predictAndShow(img, result + \".JPEG\", str(class_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
